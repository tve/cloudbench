#! /usr/bin/env ruby

require 'rubygems'
require 'right_aws'

s3_id  = ENV['AWS_ACCESS_KEY_ID']
s3_key = ENV['AWS_SECRET_ACCESS_KEY']
gcs_id  = ENV['GCS_ID']
gcs_key = ENV['GCS_KEY']

def gcs_put(bucket, path, localpath)
  data = IO.read(localpath)
  t = Time.now
  #bucket.put(file+'-old', f, {}, 'public-read')
  req = bucket.s3.interface.generate_rest_request('PUT', :url=>"#{bucket.name}/#{CGI::escape path}",
                                                  'x-amz-acl' => 'public-read', :data=>data)
  code = begin
    bucket.s3.interface.request_info(req, RightAws::RightHttp2xxParser.new)
    '200'
  rescue RightAws::AwsError => e
    puts e.message
    e.http_code
  end
  t = Time.now - t
  printf("Uploading %s took %.1f for %d bytes\n", path, t, data.size)
  "STATS #{code} #{t} #{data.size/t} 0"
end

w_fmt = "STATS %{http_code} %{time_total} %{speed_upload} %{speed_download}"

s3_buckets = %w{gcs-us gcs-eu s3-us-east s3-us-oregon s3-europe s3-ap-singapore s3-ap-tokyo s3-sa-saopaulo}

system 'dd if=/dev/urandom of=/tmp/random-10m bs=1K count=10K'
system 'dd if=/dev/urandom of=/tmp/random-10k bs=1K count=10'

# put all the "old" files in place, this should only be run when creating fresh buckets
s3_buckets.each do |bucket|
  if bucket.start_with?('gcs')
    b = RightAws::S3.new(gcs_id, gcs_key, :server => 'commondatastorage.googleapis.com').bucket(bucket)
    %w{random-10k random-10m}.each do |file|
      puts "===== Uploading #{file}-old to GCS #{bucket}"
      info = gcs_put(b, file+'-old', "/tmp/#{file}")
    end
  else
    b = RightAws::S3Generator.new(s3_id, s3_key).bucket(bucket)
    %w{random-10k random-10m}.each do |file|
      puts "===== Uploading #{file}-old to S3 #{bucket}"
      info = `curl -X PUT -T /tmp/#{file} -s -w '#{w_fmt}' '#{b.put(file+'-old')}'`
    end
  end
end if false

r={}
10.times do
  s3_buckets.each do |bucket|
    r[bucket] ||= {}
    gcs = bucket.start_with?('gcs')
    b = gcs ? RightAws::S3.new(gcs_id, gcs_key, :server => 'commondatastorage.googleapis.com') \
            : RightAws::S3Generator.new(s3_id, s3_key)
    b = b.bucket(bucket)
    [:put, :get].each do |op|
      %w{random-10k random-10m}.each do |file|
        case op
        when :put
          info = gcs ? gcs_put(b, file, "/tmp/#{file}") \
                     : `curl -X PUT -T /tmp/#{file} -s -w '#{w_fmt}' '#{b.put(file)}'`
          #puts ">> #{b.put(file)}" if !gcs
          puts "PUT #{bucket} #{file} #{info}"
          info = info.split(' ')
          r[bucket]["put-"+file] ||= Hash.new{|h,k| h[k] = []}
          r[bucket]["put-"+file][:code] << info[1] if info[1] != '200'
          r[bucket]["put-"+file][:time] << info[2].to_f if info[1] == '200'
          r[bucket]["put-"+file][:kbs]  << info[3].to_f if info[1] == '200'

        when :get
          info = gcs ? `curl -o /dev/null -s -w '#{w_fmt}' 'https://#{bucket}.commondatastorage.googleapis.com/#{file}'` \
                     : `curl -o /dev/null -s -w '#{w_fmt}' '#{b.get(file)}'`
          puts "GET #{bucket} #{file} #{info}"
          info = info.split(' ')
          r[bucket]["get-"+file] ||= Hash.new{|h,k| h[k] = []}
          r[bucket]["get-"+file][:code] << info[1] if info[1] != '200'
          r[bucket]["get-"+file][:time] << info[2].to_f if info[1] == '200'
          r[bucket]["get-"+file][:kbs]  << info[4].to_f if info[1] == '200'

        when :old
          info = gcs ? `curl -o /dev/null -s -w '#{w_fmt}' 'https://#{bucket}.commondatastorage.googleapis.com/#{file}-old'` \
                     : `curl -o /dev/null -s -w '#{w_fmt}' '#{b.get(file+"-old")}'`
          puts "GET #{bucket} #{file}-old #{info}"
          info = info.split(' ')
          r[bucket]["old-"+file] ||= Hash.new{|h,k| h[k] = []}
          r[bucket]["old-"+file][:code] << info[1] if info[1] != '200'
          r[bucket]["old-"+file][:time] << info[2].to_f if info[1] == '200'
          r[bucket]["old-"+file][:kbs]  << info[4].to_f if info[1] == '200'

        end
      end
    end
  end
end

r.keys.each do |bucket|
  r[bucket].keys.each do |op|
    errs = r[bucket][op][:code].size
    err_list = r[bucket][op][:code].join(' ')
    puts "#{bucket} #{op} => #{errs} errors: #{err_list}"
    pts = r[bucket][op][:time].size 
    if pts > 0
      time_avg = r[bucket][op][:time].inject(0.0){|s,t|s+=t} / pts
      time_95  = r[bucket][op][:time].sort[pts*95/100]
      kbs_avg  = r[bucket][op][:kbs].inject(0.0){|s,t|s+=t} / pts
      kbs_95   = r[bucket][op][:kbs].sort[pts*5/100]
      printf("%s %s => %.1f %.1f %.0f %.0fKB/s\n", bucket, op, time_avg, time_95, kbs_avg, kbs_95)

      data = {
        :machine => "#{ENV['CLOUD']}-#{ENV['MACHINE']}",
        :bucket => bucket, :op => op,
        :successes => pts, :errors => errs, 'err-list' => err_list,
        'time-avg' => time_avg, 'time-95' => time_95,
        'kbs-avg' => kbs_avg, 'kbs-95' => kbs_95 }
      puts data.inspect
      system("./gs-append storage #{data.map{|k,v| "'#{k}=#{v}'"}.join(' ')}")
    end

  end
end
