#! /usr/bin/env ruby

require 'rubygems'
require 'right_aws'
require 'google_drive'
s3_id  = ENV['AWS_ACCESS_KEY_ID']
s3_key = ENV['AWS_SECRET_ACCESS_KEY']
gcs_id  = ENV['GCS_ID']
gcs_key = ENV['GCS_KEY']

GS='0Aq-daXyC3OSSdFhqY1lORFdHSkdZMjJraTBKa0thQUE'
#$gs = GoogleDrive.saved_session
$gs = GoogleDrive::Session.new({:wise => open('.ssh_tok').gets.chomp}, nil)
$ss = $gs.spreadsheet_by_key(GS)
ws = "storage " + Time.now.strftime("%Y-%m")
puts "Spreadsheet: #{$ss.title} / #{ws}"
$sheet = $ss.worksheet_by_title(ws)
$sheet ||= $ss.add_worksheet(ws)

def append_row(gs_session, gs_sheet, row_hash={})
  xml = '<entry xmlns="http://www.w3.org/2005/Atom" xmlns:gsx="http://schemas.google.com/spreadsheets/2006/extended">'
  row_hash.each_pair do |k, v|
    k = k.to_s.gsub(/_/, '-')
    xml << "<gsx:#{k}>#{v}</gsx:#{k}>"
  end
  xml << "</entry>"
  puts xml
  gs_session.request(:post, gs_sheet.list_feed_url, :data => xml)
end

def gcs_put(bucket, path, localpath)
  puts "===== Uploading #{path} to GCS #{bucket}"
  data = IO.read(localpath)
  t = Time.now
  #bucket.put(file+'-old', f, {}, 'public-read')
  req = bucket.s3.interface.generate_rest_request('PUT', :url=>"#{bucket.name}/#{CGI::escape path}", :data=>data)
  res = bucket.s3.interface.request_info(req, RightAws::RightHttp2xxParser.new)
  t = Time.now - t
  printf("Uploading %s took %.1f for %d bytes --> %s\n", path, t, data.size, res.inspect)
  "STATS 200 #{t} #{data.size/t} 0"
end

w_fmt = "STATS %{http_code} %{time_total} %{speed_upload} %{speed_download}"

s3_buckets = %w{gcs-us gcs-eu s3-us-east s3-us-oregon s3-europe s3-ap-singapore s3-ap-tokyo s3-sa-saopaulo}

system 'dd if=/dev/urandom of=/tmp/random-10m bs=1K count=10K'
system 'dd if=/dev/urandom of=/tmp/random-10k bs=1K count=10'

# put all the "old" files in place, this should only be run when creating fresh buckets
s3_buckets.each do |bucket|
  if bucket.start_with?('gcs')
    b = RightAws::S3.new(gcs_id, gcs_key, :server => 'commondatastorage.googleapis.com').bucket(bucket)
    %w{random-10k random-10m}.each do |file|
      puts "===== Uploading #{file} to GCS #{bucket}"
      info = gcs_put(b, file+'-old', "/tmp/#{file}")
    end
  else
    b = RightAws::S3Generator.new(s3_id, s3_key).bucket(bucket)
    %w{random-10k random-10m}.each do |file|
      puts "===== Uploading #{file} to S3 #{bucket}"
      info = `curl -X PUT -T /tmp/#{file} -s -w '#{w_fmt}' '#{b.put(file+'-old')}'`
    end
  end
end if true

exit

r={}
1.times do
  s3_buckets.each do |bucket|
    r[bucket] ||= {}
    b = bucket.start_with?('gcs') ? RightAws::S3Generator.new(gcs_id, gcs_key, :server => 'commondatastorage.googleapis.com') \
                                  : RightAws::S3Generator.new(s3_id, s3_key)
    b = b.bucket(bucket)
    [:put, :old, :get].each do |op|
      %w{random-10k random-10m}.each do |file|
        case op
        when :put
          info = `curl -X PUT -T /tmp/#{file} -s -w '#{w_fmt}' '#{b.put(file)}'`
          puts "PUT #{file} #{info}"
          info = info.split(' ')
          r[bucket]["put-"+file] ||= Hash.new{|h,k| h[k] = []}
          r[bucket]["put-"+file][:code] << info[1] if info[1] != '200'
          r[bucket]["put-"+file][:time] << info[2].to_f if info[1] == '200'
          r[bucket]["put-"+file][:kbs]  << info[3].to_f if info[1] == '200'

        when :get
          info = `curl -o /dev/null -s -w '#{w_fmt}' '#{b.get(file)}'`
          puts "GET #{file} #{info}"
          info = info.split(' ')
          r[bucket]["get-"+file] ||= Hash.new{|h,k| h[k] = []}
          r[bucket]["get-"+file][:code] << info[1] if info[1] != '200'
          r[bucket]["get-"+file][:time] << info[2].to_f if info[1] == '200'
          r[bucket]["get-"+file][:kbs]  << info[4].to_f if info[1] == '200'

        when :old
          info = `curl -o /dev/null -s -w '#{w_fmt}' '#{b.get(file+"-old")}'`
          puts "GET #{file}-old #{info}"
          info = info.split(' ')
          r[bucket]["old-"+file] ||= Hash.new{|h,k| h[k] = []}
          r[bucket]["old-"+file][:code] << info[1] if info[1] != '200'
          r[bucket]["old-"+file][:time] << info[2].to_f if info[1] == '200'
          r[bucket]["old-"+file][:kbs]  << info[4].to_f if info[1] == '200'

        end
      end
    end
  end
end

r.keys.each do |bucket|
  r[bucket].keys.each do |op|
    errs = r[bucket][op][:code].size
    err_list = r[bucket][op][:code].join(' ')
    puts "#{bucket} #{op} => #{errs} errors: #{err_list}"
    pts = r[bucket][op][:time].size 
    if pts > 0
      time_avg = r[bucket][op][:time].inject(0.0){|s,t|s+=t} / pts
      time_95  = r[bucket][op][:time].sort[pts*95/100]
      kbs_avg  = r[bucket][op][:kbs].inject(0.0){|s,t|s+=t} / pts
      kbs_95   = r[bucket][op][:kbs].sort[pts*5/100]
      printf("%s %s => %.1f %.1f %.0f %.0fKB/s\n", bucket, op, time_avg, time_95, kbs_avg, kbs_95)

      data = { :cloud => ENV['CLOUD'], :machine => ENV['MACHINE'], :bucket => bucket, :op => op,
        :successes => pts, :errors => errs, :err_list => err_list,
        :time_avg => time_avg, :time_95 => time_95, :kbs_avg => kbs_avg, :kbs_95 => kbs_95 }
      append_row($gs, $sheet, data)
    end

  end
end
